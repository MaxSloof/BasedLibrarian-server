{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20d73a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#Libraries\n",
    "\n",
    "# !pip install pandoc langchain gradio chromadb tiktoken clean-text\n",
    "# !pip install \"unstructured[local-inference]\"\n",
    "# !pip install \"detectron2@git+https://github.com/facebookresearch/detectron2.git@v0.6#egg=detectron2\"\n",
    "# !pip install layoutparser pypdf unidecode\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import pandoc\n",
    "from io import StringIO\n",
    "import gradio as gr\n",
    "import re\n",
    "import time\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders import UnstructuredWordDocumentLoader\n",
    "from langchain.document_loaders import UnstructuredEPubLoader\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.embeddings import HuggingFaceEmbeddings, SentenceTransformerEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "\n",
    "from html.parser import HTMLParser\n",
    "import chromadb    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3038082f",
   "metadata": {},
   "outputs": [],
   "source": [
    "extensions = ['txt', 'md', 'pdf', 'doc', 'docx']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6189fe14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "use_triton will force moving the whole model to GPU, make sure you have enough VRAM.\n",
      "The safetensors archive passed at ../models/gpt4-x-vicuna-13B-GPTQ/GPT4-x-Vicuna-13B-GPTQ-4bit-128g-compat-act-order.safetensors does not contain metadata. Make sure to save your model with the `save_pretrained` method. Defaulting to 'pt' metadata.\n",
      "100%|██████████| 12/12 [00:29<00:00,  2.48s/it]\n"
     ]
    }
   ],
   "source": [
    "from auto_gptq.modeling import LlamaGPTQForCausalLM\n",
    "from langchain import HuggingFacePipeline\n",
    "from transformers import LlamaForCausalLM\n",
    "\n",
    "model_path = \"../models/gpt4-x-vicuna-13B-GPTQ\"\n",
    "model_name = \"GPT4-x-Vicuna-13B-GPTQ-4bit-128g-compat-act-order\"\n",
    "\n",
    "model = LlamaGPTQForCausalLM.from_quantized(model_path, model_basename= model_name, device=\"cuda:0\", use_triton=True, use_safetensors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf67b5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97b1841f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "QUESTION_PROMPT_TEMPLATE = \"\"\"### Instruction:\n",
    "Use the following portion of a long document to see if any of the text is relevant to answer the question.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "### Input:\n",
    "{context}\n",
    "\n",
    "### Response:\"\"\"\n",
    "\n",
    "QUESTION_PROMPT = PromptTemplate(template=QUESTION_PROMPT_TEMPLATE, input_variables=[\"question\",\"context\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bc4c937",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMBINE_PROMPT_TEMPLATE = \"\"\"### Instruction:\n",
    "Given the following extracted parts of a long document and a question, create a final answer. \n",
    "If you don't know the answer, just say that you don't know. Don't try to make up an answer.\n",
    "\n",
    "Question:{question}\n",
    "\n",
    "### Input:\n",
    "{summaries}\n",
    "\n",
    "### Response:\"\"\"\n",
    "COMBINE_PROMPT = PromptTemplate(template=COMBINE_PROMPT_TEMPLATE, input_variables=[\"question\",\"summaries\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2652c4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'LlamaGPTQForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "from torch.cuda import current_device\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens = 512,\n",
    "    temperature=0,\n",
    "    device=current_device()\n",
    ")\n",
    "local_llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "59cbb0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs = []\n",
    "\n",
    "def examinelibrary():\n",
    "    # Define the extensions we're looking for\n",
    "    # extensions = ['pdf']\n",
    "    all_docs.clear()\n",
    "    # Initialize a counter\n",
    "    counter = 0\n",
    "    # Scan the directory\n",
    "    for extension in extensions:\n",
    "        counter += len(glob.glob(\"../books/*.\" + extension))\n",
    "            \n",
    "    # Initialize library as dict and load documents into it\n",
    "    library = {}\n",
    "    for extension in extensions:\n",
    "        for file_name in glob.glob(\"../books/*.\" + extension):\n",
    "            content = \"\"\n",
    "            if extension in ['txt', 'md']:\n",
    "                loader = TextLoader(file_name)\n",
    "            elif extension in ['doc', 'docx']:\n",
    "                loader = UnstructuredWordDocumentLoader(file_name) \n",
    "            elif extension == 'pdf':\n",
    "                loader = PyPDFLoader(file_name)\n",
    "            elif extension == 'epub':  \n",
    "                loader = UnstructuredEPubLoader(file_name) \n",
    "        # Load the contents of the file        \n",
    "        documents = loader.load()\n",
    "        # Add the file and its contents to the library\n",
    "        library[file_name] = documents\n",
    "        \n",
    "    # Display the count of files detected   \n",
    "    return(library, counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2af0c9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the texts in the library into chunks\n",
    "def processtext(library):\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "    texts = []\n",
    "    for file_name in library:\n",
    "          texts += text_splitter.split_documents(library[file_name])\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c8672e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce a librarian (database of embedded texts)\n",
    "def prodlibrarian():\n",
    "    library,counter = examinelibrary()\n",
    "    texts = processtext(library)\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"intfloat/e5-large-v2\")\n",
    "    # embeddings = OpenAIEmbeddings(openai_api_base=OPENAI_API_BASE)\n",
    "    db = Chroma.from_documents(texts, embeddings)\n",
    "    return (db, counter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9c2ec702",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function DuckDB.__del__ at 0x7fbecfaf96c0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/chromadb/db/duckdb.py\", line 387, in __del__\n",
      "    self.reset_indexes()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/chromadb/db/clickhouse.py\", line 610, in reset_indexes\n",
      "    delete_all_indexes(self._settings)\n",
      "AttributeError: 'DuckDB' object has no attribute '_settings'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convert /app/books/mediations-marcus-aurelius.doc -> /tmp/tmpguv1_g7j/mediations-marcus-aurelius.docx using filter : MS Word 2007 XML\n",
      "convert /app/books/mediations-marcus-aurelius.doc -> /tmp/tmpfo2d9o_o/mediations-marcus-aurelius.docx using filter : MS Word 2007 XML\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1999, which is longer than the specified 1000\n",
      "Created a chunk of size 1358, which is longer than the specified 1000\n",
      "Created a chunk of size 1121, which is longer than the specified 1000\n",
      "Created a chunk of size 1124, which is longer than the specified 1000\n",
      "Created a chunk of size 1414, which is longer than the specified 1000\n",
      "Created a chunk of size 1213, which is longer than the specified 1000\n",
      "Created a chunk of size 1761, which is longer than the specified 1000\n",
      "Created a chunk of size 1007, which is longer than the specified 1000\n",
      "Created a chunk of size 1100, which is longer than the specified 1000\n",
      "Created a chunk of size 1134, which is longer than the specified 1000\n",
      "Created a chunk of size 1168, which is longer than the specified 1000\n",
      "Created a chunk of size 1394, which is longer than the specified 1000\n",
      "Created a chunk of size 2411, which is longer than the specified 1000\n",
      "Created a chunk of size 1079, which is longer than the specified 1000\n",
      "Created a chunk of size 1070, which is longer than the specified 1000\n",
      "Created a chunk of size 1052, which is longer than the specified 1000\n",
      "Created a chunk of size 1741, which is longer than the specified 1000\n",
      "Created a chunk of size 1068, which is longer than the specified 1000\n",
      "Created a chunk of size 1088, which is longer than the specified 1000\n",
      "Created a chunk of size 1100, which is longer than the specified 1000\n",
      "Created a chunk of size 2098, which is longer than the specified 1000\n",
      "Created a chunk of size 1644, which is longer than the specified 1000\n",
      "Created a chunk of size 1385, which is longer than the specified 1000\n",
      "Created a chunk of size 1759, which is longer than the specified 1000\n",
      "Created a chunk of size 1300, which is longer than the specified 1000\n",
      "Created a chunk of size 1062, which is longer than the specified 1000\n",
      "Created a chunk of size 1067, which is longer than the specified 1000\n",
      "Created a chunk of size 1457, which is longer than the specified 1000\n",
      "Created a chunk of size 1034, which is longer than the specified 1000\n",
      "Created a chunk of size 1382, which is longer than the specified 1000\n",
      "Created a chunk of size 1845, which is longer than the specified 1000\n",
      "Created a chunk of size 1042, which is longer than the specified 1000\n",
      "Created a chunk of size 1027, which is longer than the specified 1000\n",
      "Created a chunk of size 1121, which is longer than the specified 1000\n",
      "Created a chunk of size 1354, which is longer than the specified 1000\n",
      "Created a chunk of size 1311, which is longer than the specified 1000\n",
      "Created a chunk of size 1389, which is longer than the specified 1000\n",
      "Created a chunk of size 1496, which is longer than the specified 1000\n",
      "Created a chunk of size 1496, which is longer than the specified 1000\n",
      "Created a chunk of size 3076, which is longer than the specified 1000\n",
      "Created a chunk of size 1526, which is longer than the specified 1000\n",
      "Created a chunk of size 1278, which is longer than the specified 1000\n",
      "Created a chunk of size 1709, which is longer than the specified 1000\n",
      "Created a chunk of size 1184, which is longer than the specified 1000\n",
      "Created a chunk of size 1063, which is longer than the specified 1000\n",
      "Created a chunk of size 1955, which is longer than the specified 1000\n",
      "Created a chunk of size 1096, which is longer than the specified 1000\n",
      "Created a chunk of size 1347, which is longer than the specified 1000\n",
      "Created a chunk of size 1187, which is longer than the specified 1000\n",
      "Created a chunk of size 1087, which is longer than the specified 1000\n",
      "Created a chunk of size 1692, which is longer than the specified 1000\n",
      "Created a chunk of size 2164, which is longer than the specified 1000\n",
      "Created a chunk of size 1009, which is longer than the specified 1000\n",
      "Created a chunk of size 1325, which is longer than the specified 1000\n",
      "Created a chunk of size 1268, which is longer than the specified 1000\n",
      "Created a chunk of size 1596, which is longer than the specified 1000\n",
      "Created a chunk of size 1930, which is longer than the specified 1000\n",
      "Created a chunk of size 1293, which is longer than the specified 1000\n",
      "Created a chunk of size 1154, which is longer than the specified 1000\n",
      "Created a chunk of size 1124, which is longer than the specified 1000\n",
      "Created a chunk of size 1029, which is longer than the specified 1000\n",
      "Created a chunk of size 1655, which is longer than the specified 1000\n",
      "Created a chunk of size 1429, which is longer than the specified 1000\n",
      "Created a chunk of size 1710, which is longer than the specified 1000\n",
      "Created a chunk of size 2401, which is longer than the specified 1000\n",
      "Created a chunk of size 1598, which is longer than the specified 1000\n",
      "Created a chunk of size 1560, which is longer than the specified 1000\n",
      "Created a chunk of size 1071, which is longer than the specified 1000\n",
      "Created a chunk of size 1263, which is longer than the specified 1000\n",
      "Created a chunk of size 1326, which is longer than the specified 1000\n",
      "Created a chunk of size 1274, which is longer than the specified 1000\n",
      "Created a chunk of size 1325, which is longer than the specified 1000\n",
      "Created a chunk of size 1162, which is longer than the specified 1000\n",
      "Created a chunk of size 1145, which is longer than the specified 1000\n",
      "Created a chunk of size 1174, which is longer than the specified 1000\n",
      "Created a chunk of size 1018, which is longer than the specified 1000\n",
      "Created a chunk of size 1591, which is longer than the specified 1000\n",
      "Created a chunk of size 2878, which is longer than the specified 1000\n",
      "Created a chunk of size 1839, which is longer than the specified 1000\n",
      "Created a chunk of size 1320, which is longer than the specified 1000\n",
      "Created a chunk of size 1567, which is longer than the specified 1000\n",
      "Created a chunk of size 1564, which is longer than the specified 1000\n",
      "Created a chunk of size 3755, which is longer than the specified 1000\n",
      "Created a chunk of size 2424, which is longer than the specified 1000\n",
      "Created a chunk of size 2184, which is longer than the specified 1000\n",
      "Created a chunk of size 1788, which is longer than the specified 1000\n",
      "Created a chunk of size 3176, which is longer than the specified 1000\n",
      "Created a chunk of size 1985, which is longer than the specified 1000\n",
      "Created a chunk of size 1679, which is longer than the specified 1000\n",
      "Created a chunk of size 3132, which is longer than the specified 1000\n",
      "Created a chunk of size 4300, which is longer than the specified 1000\n",
      "Created a chunk of size 1873, which is longer than the specified 1000\n",
      "Created a chunk of size 1631, which is longer than the specified 1000\n",
      "Created a chunk of size 4810, which is longer than the specified 1000\n",
      "Created a chunk of size 1218, which is longer than the specified 1000\n",
      "Created a chunk of size 3884, which is longer than the specified 1000\n",
      "Created a chunk of size 8216, which is longer than the specified 1000\n",
      "Created a chunk of size 1189, which is longer than the specified 1000\n",
      "Created a chunk of size 1055, which is longer than the specified 1000\n",
      "Created a chunk of size 2178, which is longer than the specified 1000\n",
      "Created a chunk of size 2624, which is longer than the specified 1000\n",
      "Created a chunk of size 1203, which is longer than the specified 1000\n",
      "Created a chunk of size 1806, which is longer than the specified 1000\n",
      "Created a chunk of size 1992, which is longer than the specified 1000\n",
      "Created a chunk of size 1144, which is longer than the specified 1000\n",
      "Created a chunk of size 2893, which is longer than the specified 1000\n",
      "Created a chunk of size 2591, which is longer than the specified 1000\n",
      "Created a chunk of size 1820, which is longer than the specified 1000\n",
      "Created a chunk of size 1225, which is longer than the specified 1000\n",
      "Created a chunk of size 1062, which is longer than the specified 1000\n",
      "Created a chunk of size 1215, which is longer than the specified 1000\n",
      "Created a chunk of size 1039, which is longer than the specified 1000\n",
      "Created a chunk of size 1104, which is longer than the specified 1000\n",
      "Created a chunk of size 1047, which is longer than the specified 1000\n",
      "Created a chunk of size 5276, which is longer than the specified 1000\n",
      "Created a chunk of size 4177, which is longer than the specified 1000\n",
      "Created a chunk of size 1153, which is longer than the specified 1000\n",
      "Created a chunk of size 1183, which is longer than the specified 1000\n",
      "Created a chunk of size 1708, which is longer than the specified 1000\n",
      "Created a chunk of size 1243, which is longer than the specified 1000\n",
      "Created a chunk of size 1521, which is longer than the specified 1000\n",
      "Created a chunk of size 1104, which is longer than the specified 1000\n",
      "Created a chunk of size 1380, which is longer than the specified 1000\n",
      "Created a chunk of size 1301, which is longer than the specified 1000\n",
      "Created a chunk of size 1877, which is longer than the specified 1000\n",
      "Created a chunk of size 1164, which is longer than the specified 1000\n",
      "Created a chunk of size 3336, which is longer than the specified 1000\n",
      "Created a chunk of size 2165, which is longer than the specified 1000\n",
      "Created a chunk of size 1127, which is longer than the specified 1000\n",
      "Created a chunk of size 1082, which is longer than the specified 1000\n",
      "Created a chunk of size 1380, which is longer than the specified 1000\n",
      "Created a chunk of size 4000, which is longer than the specified 1000\n",
      "Created a chunk of size 1026, which is longer than the specified 1000\n",
      "Created a chunk of size 1725, which is longer than the specified 1000\n",
      "Created a chunk of size 1407, which is longer than the specified 1000\n",
      "Created a chunk of size 1004, which is longer than the specified 1000\n",
      "Created a chunk of size 1021, which is longer than the specified 1000\n",
      "Created a chunk of size 1448, which is longer than the specified 1000\n",
      "Created a chunk of size 1272, which is longer than the specified 1000\n",
      "Created a chunk of size 1607, which is longer than the specified 1000\n",
      "Created a chunk of size 1296, which is longer than the specified 1000\n",
      "Created a chunk of size 1031, which is longer than the specified 1000\n",
      "Created a chunk of size 1978, which is longer than the specified 1000\n",
      "Created a chunk of size 1290, which is longer than the specified 1000\n",
      "Created a chunk of size 1854, which is longer than the specified 1000\n",
      "Created a chunk of size 3321, which is longer than the specified 1000\n",
      "Created a chunk of size 3045, which is longer than the specified 1000\n",
      "Created a chunk of size 1379, which is longer than the specified 1000\n",
      "Created a chunk of size 1581, which is longer than the specified 1000\n",
      "Created a chunk of size 1177, which is longer than the specified 1000\n",
      "Created a chunk of size 1018, which is longer than the specified 1000\n",
      "Created a chunk of size 1812, which is longer than the specified 1000\n",
      "Created a chunk of size 1306, which is longer than the specified 1000\n",
      "Created a chunk of size 1031, which is longer than the specified 1000\n",
      "Created a chunk of size 1333, which is longer than the specified 1000\n",
      "Created a chunk of size 1094, which is longer than the specified 1000\n",
      "Created a chunk of size 1041, which is longer than the specified 1000\n",
      "Created a chunk of size 2365, which is longer than the specified 1000\n",
      "Created a chunk of size 1641, which is longer than the specified 1000\n",
      "Created a chunk of size 1024, which is longer than the specified 1000\n",
      "Created a chunk of size 1816, which is longer than the specified 1000\n",
      "Created a chunk of size 1250, which is longer than the specified 1000\n",
      "Created a chunk of size 1475, which is longer than the specified 1000\n",
      "Created a chunk of size 1142, which is longer than the specified 1000\n",
      "Created a chunk of size 1239, which is longer than the specified 1000\n",
      "Created a chunk of size 1574, which is longer than the specified 1000\n",
      "Created a chunk of size 1047, which is longer than the specified 1000\n",
      "Created a chunk of size 1084, which is longer than the specified 1000\n",
      "Created a chunk of size 2296, which is longer than the specified 1000\n",
      "Created a chunk of size 1139, which is longer than the specified 1000\n",
      "Created a chunk of size 1549, which is longer than the specified 1000\n",
      "Created a chunk of size 1453, which is longer than the specified 1000\n",
      "Created a chunk of size 1051, which is longer than the specified 1000\n",
      "Created a chunk of size 1585, which is longer than the specified 1000\n",
      "Created a chunk of size 1366, which is longer than the specified 1000\n",
      "Created a chunk of size 1854, which is longer than the specified 1000\n",
      "Created a chunk of size 1391, which is longer than the specified 1000\n",
      "Created a chunk of size 1138, which is longer than the specified 1000\n",
      "Created a chunk of size 1286, which is longer than the specified 1000\n",
      "Created a chunk of size 1113, which is longer than the specified 1000\n",
      "Created a chunk of size 1470, which is longer than the specified 1000\n",
      "Created a chunk of size 1174, which is longer than the specified 1000\n",
      "Created a chunk of size 1029, which is longer than the specified 1000\n",
      "Created a chunk of size 3278, which is longer than the specified 1000\n",
      "Created a chunk of size 1977, which is longer than the specified 1000\n",
      "Created a chunk of size 2728, which is longer than the specified 1000\n",
      "Created a chunk of size 1292, which is longer than the specified 1000\n",
      "Created a chunk of size 1296, which is longer than the specified 1000\n",
      "Created a chunk of size 1670, which is longer than the specified 1000\n",
      "Created a chunk of size 1231, which is longer than the specified 1000\n",
      "Created a chunk of size 3159, which is longer than the specified 1000\n",
      "Created a chunk of size 1481, which is longer than the specified 1000\n",
      "Created a chunk of size 1859, which is longer than the specified 1000\n",
      "Created a chunk of size 2883, which is longer than the specified 1000\n",
      "Created a chunk of size 3332, which is longer than the specified 1000\n",
      "Created a chunk of size 1450, which is longer than the specified 1000\n",
      "Created a chunk of size 1152, which is longer than the specified 1000\n",
      "Created a chunk of size 3085, which is longer than the specified 1000\n",
      "Created a chunk of size 1616, which is longer than the specified 1000\n",
      "Created a chunk of size 1734, which is longer than the specified 1000\n",
      "Created a chunk of size 1996, which is longer than the specified 1000\n",
      "Created a chunk of size 1671, which is longer than the specified 1000\n",
      "Created a chunk of size 1003, which is longer than the specified 1000\n",
      "Created a chunk of size 1143, which is longer than the specified 1000\n",
      "Created a chunk of size 5834, which is longer than the specified 1000\n",
      "Created a chunk of size 1620, which is longer than the specified 1000\n",
      "Created a chunk of size 1097, which is longer than the specified 1000\n",
      "Created a chunk of size 1494, which is longer than the specified 1000\n",
      "Created a chunk of size 2031, which is longer than the specified 1000\n",
      "Created a chunk of size 1181, which is longer than the specified 1000\n",
      "Created a chunk of size 1164, which is longer than the specified 1000\n",
      "Created a chunk of size 1013, which is longer than the specified 1000\n",
      "Created a chunk of size 1592, which is longer than the specified 1000\n",
      "Created a chunk of size 1151, which is longer than the specified 1000\n",
      "Created a chunk of size 1013, which is longer than the specified 1000\n",
      "Created a chunk of size 1091, which is longer than the specified 1000\n",
      "Created a chunk of size 1049, which is longer than the specified 1000\n",
      "Created a chunk of size 1550, which is longer than the specified 1000\n",
      "Created a chunk of size 1741, which is longer than the specified 1000\n",
      "Created a chunk of size 1061, which is longer than the specified 1000\n",
      "Created a chunk of size 1726, which is longer than the specified 1000\n",
      "Created a chunk of size 1125, which is longer than the specified 1000\n",
      "Created a chunk of size 1171, which is longer than the specified 1000\n",
      "Created a chunk of size 1093, which is longer than the specified 1000\n",
      "Created a chunk of size 1537, which is longer than the specified 1000\n",
      "Created a chunk of size 1837, which is longer than the specified 1000\n",
      "Created a chunk of size 1899, which is longer than the specified 1000\n",
      "Created a chunk of size 2008, which is longer than the specified 1000\n",
      "Created a chunk of size 1314, which is longer than the specified 1000\n",
      "Created a chunk of size 1212, which is longer than the specified 1000\n",
      "Created a chunk of size 1780, which is longer than the specified 1000\n",
      "Created a chunk of size 1185, which is longer than the specified 1000\n",
      "Created a chunk of size 1046, which is longer than the specified 1000\n",
      "No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/intfloat_e5-large-v2. Creating a new one with MEAN pooling.\n",
      "Using embedded DuckDB without persistence: data will be transient\n"
     ]
    }
   ],
   "source": [
    "# Init on first run\n",
    "db,counter = prodlibrarian()\n",
    "\n",
    "\n",
    "# Call prodlibrarian and display progress\n",
    "def scan(progress=gr.Progress()):\n",
    "    progress(0.2, desc=\"Examining /books folder ...\")\n",
    "    time.sleep(1)\n",
    "    progress(0.4, desc=\"Counting documents ...\")\n",
    "    time.sleep(1.5)\n",
    "    progress(0.6, desc=\"Generating embeddings ...\")\n",
    "    time.sleep(1.5)\n",
    "    global db\n",
    "    db, counter = prodlibrarian()\n",
    "    return \"Librarian found and embedded \" + str(counter) + \" documents.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d3c37b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_metadata_page(docs):\n",
    "    for i in range(len(docs)):\n",
    "        if 'page' not in docs[i].metadata.keys():\n",
    "            docs[i].metadata['page'] = \"Not applicable\"\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ad5d14f2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7869\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7869/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize language model and qa chain\n",
    "# llm = OpenAI(temperature=0, openai_api_base=OPENAI_API_BASE, model=\"gpt4-x-vicuna-13B-GPTQ\")\n",
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "from time import perf_counter\n",
    "\n",
    "# chain_type_kwargs = {\"prompt\": prompt}\n",
    "chain = load_qa_with_sources_chain(local_llm, chain_type=\"map_reduce\", question_prompt=QUESTION_PROMPT, combine_prompt=COMBINE_PROMPT,verbose=True)\n",
    "\n",
    "# Main query code.\n",
    "def ask(query, search_filter=None, progress=gr.Progress()):\n",
    "    start_time = perf_counter()\n",
    "\n",
    "    progress(0.1, desc=\"Scanning embedded documents for matches ...\")\n",
    "  \n",
    "    #generate docs which are texts relevant to the query\n",
    "    docs = db.similarity_search(query, k=2, filter=search_filter) \n",
    "\n",
    "    docs = check_metadata_page(docs)\n",
    "\n",
    "    progress(0.2, desc=\"Assembling request...\")\n",
    "    progress(0.4, desc=\"Appending citations and metadata ...\")\n",
    "    #some muckwork to log sources\n",
    "    x = 0\n",
    "    citations = \"\"\n",
    "    for x in range(len(docs)): \n",
    "        # citations += docs[x].metadata['source']  + \"\\n\"\n",
    "        citations += docs[x].metadata['source'] + \" in page: \" + str(docs[x].metadata['page'])  + \"\\n\"\n",
    "    \n",
    "    progress(0.5, desc=\"Talking to LLM for answers ...\")\n",
    "\n",
    "    #calls llm with the query and relevant docs in hand and returns both the response and the sources    \n",
    "    librarianoutput = chain({\"input_documents\":docs, \"question\": query})\n",
    "    output = \"Answer: \\n\" + librarianoutput[\"output_text\"] + \"\\n\\nI found this in: \\n\" + citations\n",
    "\n",
    "    query_time = str(round(perf_counter() - start_time ,2)) + \" seconds\"\n",
    "    return (output, query_time)\n",
    "\n",
    "#Gradio UI\n",
    "with gr.Blocks() as app:\n",
    "\n",
    "\n",
    "    with gr.Row():\n",
    "        gr.Markdown(\"# Welcome to your Natassistant!\")\n",
    "        scan_btn = gr.Button(\"Scan the library again.\")\n",
    "        \n",
    "    query = gr.Textbox(label=\"What can I help you find?\")\n",
    "    output = gr.Textbox(label=\"Response:\")\n",
    "    ask_btn = gr.Button(\"Ask Librarian\")\n",
    "    # performance_box = gr.Textbox(label=f\"Time to complete query:\",)\n",
    "    selected_docs = gr.CheckboxGroup(all_docs, label=\"Documents used as input for the model\")\n",
    "\n",
    "    ask_btn.click(fn=ask, inputs=query, outputs=output)\n",
    "    scan_btn.click(fn=scan, outputs=output)\n",
    "    # gr.Markdown(\"*...a private library is not an ego-boosting appendage but a research tool. The library should contain as much of what you do not know ... You will accumulate more knowledge and more books as you grow older, and the growing number of unread books on the shelves will look at you menacingly. Indeed, the more you know, the larger the rows of unread books. Let us call this collection of unread books an antilibrary.* \\n - Nassim Nicholas Taleb, The Black Swan\")\n",
    "app.queue(concurrency_count=1).launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6ff96e2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': '../books/the-prince-machiavelli.txt'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.search(\"Machiavelli\" ,\"mmr\")[0].dict()['metadata']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d623764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize language model and qa chain\n",
    "# # llm = OpenAI(temperature=0, openai_api_base=OPENAI_API_BASE, model=\"gpt4-x-vicuna-13B-GPTQ\")\n",
    "# from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "\n",
    "# # chain_type_kwargs = {\"prompt\": prompt}\n",
    "# chain = load_qa_with_sources_chain(local_llm, chain_type=\"map_reduce\", question_prompt=prompt)\n",
    "\n",
    "# # Main query code.\n",
    "# def ask(query):\n",
    "#     # progress(0.1, desc=\"Scanning embedded documents for matches ...\")\n",
    "#     # time.sleep(1)\n",
    "#     # progress(0.2, desc=\"Assembling request...\")\n",
    "#     # time.sleep(1.5)\n",
    "#     # progress(0.4, desc=\"Appending citations and metadata ...\")\n",
    "#     # time.sleep(1.5)\n",
    "#     # progress(0.5, desc=\"Talking to LLM for answers ...\")\n",
    "#     # time.sleep(1.5)\n",
    "#     #generate docs which are texts relevant to the query\n",
    "#     print(\"Similarity search!\")\n",
    "#     docs = db.similarity_search(query,k=2) \n",
    "#     #some muckwork to log sources\n",
    "#     x = 0\n",
    "#     citations = \"\"\n",
    "\n",
    "#     print(\"Collecting relevant documents\")\n",
    "#     for x in range(len(docs)): \n",
    "#         citations += docs[x].metadata['source']  + \"\\n\"\n",
    "#         citations += docs[x].metadata['source'] + \" in page: \" + str(docs[x].metadata['page'])  + \"\\n\"\n",
    "    \n",
    "#     print(\"Asking the LLM\")\n",
    "#     #calls llm with the query and relevant docs in hand and returns both the response and the sources    \n",
    "#     librarianoutput = chain({\"input_documents\":docs, \"question\": query})\n",
    "#     # output = \"Answer: \\n\" + librarianoutput + \"\\n\\nI found this in: \\n\" + citations\n",
    "#     return (librarianoutput[\"output_text\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6bc8e9bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_llm.get_num_tokens(\"The conclusion of the document states that a distributed infrastructure has been used to train and apply large-scale language models to machine translation. The results show that increasing the amount of training data up to 2 trillion tokens results in a 5-gram language model of up to 300 billion n-grams. The technique is made efficient by batching score requests by the decoder in a server-client architecture. A simple smoothing technique suitable for distributed computation was proposed and performed as well as more sophisticated methods. Additionally, translation quality, as indicated by BLEU score, continued to improve with increasing language model size at even the largest sizes considered.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
